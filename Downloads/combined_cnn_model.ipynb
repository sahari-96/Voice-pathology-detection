{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcHoU6W0xeL6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Create the first input branch for mfcc features\n",
        "input1 = layers.Input(shape=(number_of_frames, 13, 1))\n",
        "x1 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input1)  # Convolutional layer with ReLU activation\n",
        "x1 = layers.MaxPooling2D(pool_size=(2, 2))(x1)  # Max pooling layer\n",
        "x1 = layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x1)  # Convolutional layer with ReLU activation\n",
        "x1 = layers.MaxPooling2D(pool_size=(2, 2))(x1)  # Max pooling layer\n",
        "x1 = layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x1)  # Convolutional layer with ReLU activation\n",
        "x1 = layers.MaxPooling2D(pool_size=(2, 2))(x1)  # Max pooling layer\n",
        "x1 = layers.GlobalAveragePooling2D()(x1)  # Global average pooling layer\n",
        "\n",
        "# Create the second input branch for phone posterior probabilities features\n",
        "input2 = layers.Input(shape=(number_of_frames, 32, 1))\n",
        "x2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input2)  # Convolutional layer with ReLU activation\n",
        "x2 = layers.MaxPooling2D(pool_size=(2, 2))(x2)  # Max pooling layer\n",
        "x2 = layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x2)  # Convolutional layer with ReLU activation\n",
        "x2 = layers.MaxPooling2D(pool_size=(2, 2))(x2)  # Max pooling layer\n",
        "x2 = layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x2)  # Convolutional layer with ReLU activation\n",
        "x2 = layers.MaxPooling2D(pool_size=(2, 2))(x2)  # Max pooling layer\n",
        "x2 = layers.GlobalAveragePooling2D()(x2)  # Global average pooling layer\n",
        "\n",
        "# Merge the global average pooling layer output for branches\n",
        "merged = layers.concatenate([x1, x2])  # Concatenate the outputs of the two branches\n",
        "\n",
        "# Add additional Dense layer and dropout\n",
        "merged = layers.Dropout(0.5)(merged)  # Dropout layer for regularization\n",
        "merged = layers.Dense(128, activation='relu')(merged)  # Fully connected layer with ReLU activation\n",
        "merged = layers.Dropout(0.3)(merged)  # Dropout layer for regularization\n",
        "\n",
        "# Add the final classification layer\n",
        "output = layers.Dense(1, activation='sigmoid')(merged)  # Output layer with sigmoid activation\n",
        "\n",
        "# Create the model\n",
        "model = models.Model(inputs=[input1, input2], outputs=output)  # Define the input and output of the model\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Compile the model with Adam optimizer and binary crossentropy loss\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()  # Display model architecture summary"
      ]
    }
  ]
}