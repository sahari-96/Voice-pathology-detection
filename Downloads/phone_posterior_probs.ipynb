{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIk89bKQkVoi"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import json\n",
        "import librosa\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "# Install and import necessary libraries\n",
        "!pip install transformers\n",
        "\n",
        "# Load pretrained model and processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "def process_audio(audio_path):\n",
        "    \"\"\"\n",
        "    Process audio file to extract input values for model.\n",
        "\n",
        "    Args:\n",
        "    audio_path (str): Path to the audio file.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: Input values for the model.\n",
        "    \"\"\"\n",
        "    target_duration = 83  # seconds\n",
        "\n",
        "    # Load the audio waveform and resample if necessary\n",
        "    waveform, sample_rate = librosa.load(audio_path, sr=16000, mono=True)\n",
        "\n",
        "    # Extract the desired segment\n",
        "    segment = waveform[:int(target_duration * sample_rate)]\n",
        "    input_values = processor(waveform, return_tensors=\"pt\").input_values.squeeze()\n",
        "    return input_values\n",
        "\n",
        "def get_phone_posteriors(audio_path):\n",
        "    \"\"\"\n",
        "    Get phone posteriors from audio file.\n",
        "\n",
        "    Args:\n",
        "    audio_path (str): Path to the audio file.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: Phone posteriors.\n",
        "    \"\"\"\n",
        "    input_values = process_audio(audio_path)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values.unsqueeze(0)).logits\n",
        "        posteriors = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    phone_posteriors = posteriors[0]\n",
        "    return phone_posteriors\n",
        "\n",
        "# Process each audio file in the specified directory\n",
        "a1 = []\n",
        "for file in glob.glob(\"/content/drive/MyDrive/DATA/11/train/*.wav\"):\n",
        "    posterior_probs = get_phone_posteriors(file)\n",
        "    tensor_list = posterior_probs.squeeze().tolist()\n",
        "    filename1 = Path(file).stem\n",
        "    x1 = filename1.split(\"_\")\n",
        "    a1.append([x1[0], tensor_list])\n",
        "\n",
        "# Save the results to a JSON file\n",
        "file_path = '/content/a1.json'\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(a1, json_file)"
      ]
    }
  ]
}